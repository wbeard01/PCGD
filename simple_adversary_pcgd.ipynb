{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wbeard01/PCGD/blob/main/simple_adversary_pcgd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbLFTuU7rlaW",
        "outputId": "cf6017ee-2bc8-44f9-e009-e0206bc00765"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pettingzoo\n",
            "  Downloading pettingzoo-1.24.1-py3-none-any.whl (840 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/840.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/840.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m839.7/840.8 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.8/840.8 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pettingzoo) (1.23.5)\n",
            "Collecting gymnasium>=0.28.0 (from pettingzoo)\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo) (4.5.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium>=0.28.0->pettingzoo)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium, pettingzoo\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1 pettingzoo-1.24.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pettingzoo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXac27B2rr2x"
      },
      "outputs": [],
      "source": [
        "from pettingzoo.mpe import simple_adversary_v3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJEHNDbrONel"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.sparse.linalg import gmres\n",
        "from scipy.sparse.linalg import LinearOperator\n",
        "import pandas as pd\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Np-QEgcOSUg",
        "outputId": "99fe10f4-21ad-46ea-bb69-47d99015b22a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "class PolicyPi(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=128):\n",
        "        super().__init__()\n",
        "\n",
        "        self.firstHidden = nn.Linear(input_dim, hidden_dim)\n",
        "        self.secondHidden = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.thirdHidden = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.classify = nn.Linear(hidden_dim, 5)\n",
        "\n",
        "    def forward(self, s):\n",
        "        outs = self.firstHidden(s)\n",
        "        outs = F.relu(outs)\n",
        "        outs = self.secondHidden(outs)\n",
        "        outs = F.relu(outs)\n",
        "        outs = self.thirdHidden(outs)\n",
        "        outs = F.relu(outs)\n",
        "        logits = self.classify(outs)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZsrinkW9WtA"
      },
      "outputs": [],
      "source": [
        "class SimGD:\n",
        "  def __init__(self, policy, lr):\n",
        "    self.policy = policy\n",
        "    self.lr = lr\n",
        "\n",
        "  def zero_grad(self):\n",
        "      for param in self.policy.parameters():\n",
        "          if param.grad is not None:\n",
        "              param.grad.detach()\n",
        "              param.grad.zero_()\n",
        "\n",
        "  def step(self, loss):\n",
        "      grads = torch.autograd.grad(loss, self.policy.parameters())\n",
        "      for param, grad in zip(self.policy.parameters(), grads):\n",
        "          param.data -= self.lr * grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykNP2PaJ0dA4"
      },
      "outputs": [],
      "source": [
        "class PCGD:\n",
        "    def __init__(self, policies, eta):\n",
        "        self.policies = policies\n",
        "        self.eta = eta\n",
        "        self.agents = [\"adversary_0\", \"agent_0\", \"agent_1\"]\n",
        "\n",
        "    def zero_grad(self):\n",
        "      for agent in self.agents:\n",
        "          for param in self.policies[agent].parameters():\n",
        "              if param.grad is not None:\n",
        "                  param.grad.detach()\n",
        "                  param.grad.zero_()\n",
        "\n",
        "    def custom_flatten(self, gp):\n",
        "        flattened = []\n",
        "        for g in gp:\n",
        "            flattened.append(g.flatten())\n",
        "        return torch.concat(flattened)\n",
        "\n",
        "    def loss_matrix(self, log_probs, cum_rewards):\n",
        "        agents = self.agents\n",
        "        losses = torch.zeros((len(agents), len(agents)))\n",
        "        for row in range(len(agents)):\n",
        "            for col in range(len(agents)):\n",
        "                losses[row, col] = (cum_rewards[agents[row]] * log_probs[agents[col]] * log_probs[agents[row]]).mean()\n",
        "        return losses\n",
        "\n",
        "    def zeta(self, log_probs, cum_rewards):\n",
        "        self.zero_grad()\n",
        "        agents = self.agents\n",
        "        zeta = []\n",
        "        for row in agents:\n",
        "            reward = (log_probs[row] * cum_rewards[row]).mean()\n",
        "            grads = torch.autograd.grad(reward, self.policies[row].parameters(), retain_graph=True, create_graph=True)\n",
        "            zeta.append(self.custom_flatten(grads))\n",
        "        return torch.concat(zeta)\n",
        "\n",
        "    def mvp(self, loss_mat, vec):\n",
        "        self.zero_grad()\n",
        "        vec = vec.reshape(-1, 1)\n",
        "        agents = self.agents\n",
        "        split = sum(p.numel() for p in self.policies[agents[0]].parameters())\n",
        "        split2 = sum(p.numel() for p in self.policies[agents[1]].parameters())\n",
        "        blocks = [vec[:split], vec[split:split+split2], vec[split+split2:]]\n",
        "        new_blocks = []\n",
        "        for row in range(len(agents)):\n",
        "            acc = blocks[row].clone()\n",
        "            for col in range(len(agents)):\n",
        "                if row != col:\n",
        "                    reward = loss_mat[row, col]\n",
        "                    grads = self.custom_flatten(torch.autograd.grad(reward, self.policies[agents[col]].parameters(), retain_graph=True, create_graph=True)).reshape(-1, 1)\n",
        "                    vjp = self.custom_flatten(torch.autograd.grad(grads, self.policies[agents[row]].parameters(), [blocks[col]], retain_graph=True)).reshape(-1, 1)\n",
        "                    acc += self.eta * vjp\n",
        "            new_blocks.append(acc)\n",
        "        return torch.concat(new_blocks)\n",
        "\n",
        "    def compute_loss_mat_update_iterative(self, loss_mat, zeta):\n",
        "        mv = lambda v: self.mvp(loss_mat, torch.tensor(v)).detach().numpy()\n",
        "        A = LinearOperator((zeta.shape[0], zeta.shape[0]), matvec=mv)\n",
        "        b = zeta.detach().numpy()\n",
        "        return self.eta * torch.tensor(gmres(A, b)[0])\n",
        "\n",
        "    def update_parameters(self, update):\n",
        "        # MAGIC TO SAFELY UPDATE PARAMETERS\n",
        "        agents = self.agents\n",
        "        split = sum(p.numel() for p in self.policies[agents[0]].parameters())\n",
        "        split2 = sum(p.numel() for p in self.policies[agents[1]].parameters())\n",
        "        first = update[:split]\n",
        "        second = update[split:split+split2]\n",
        "        third = update[split+split2:]\n",
        "        grad_like_policy = []\n",
        "        idx = 0\n",
        "        for param in self.policies[agents[0]].parameters():\n",
        "            grad_like_policy.append(first[idx : idx + torch.numel(param)].reshape(param.shape))\n",
        "            idx += torch.numel(param)\n",
        "        for param, grad in zip(self.policies[agents[0]].parameters(), grad_like_policy):\n",
        "            param.data += grad\n",
        "\n",
        "        grad_like_policy = []\n",
        "        idx = 0\n",
        "        for param in self.policies[agents[1]].parameters():\n",
        "            grad_like_policy.append(second[idx : idx + torch.numel(param)].reshape(param.shape))\n",
        "            idx += torch.numel(param)\n",
        "        for param, grad in zip(self.policies[agents[1]].parameters(), grad_like_policy):\n",
        "            param.data += grad\n",
        "\n",
        "        grad_like_policy = []\n",
        "        idx = 0\n",
        "        for param in self.policies[agents[2]].parameters():\n",
        "            grad_like_policy.append(third[idx : idx + torch.numel(param)].reshape(param.shape))\n",
        "            idx += torch.numel(param)\n",
        "        for param, grad in zip(self.policies[agents[2]].parameters(), grad_like_policy):\n",
        "            param.data += grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 883
        },
        "id": "U6_HlH9hsWUU",
        "outputId": "1932c011-f071-48d2-dedd-ec011b43db16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<decorator-gen-5>:2: DeprecationWarning: scipy.sparse.linalg.gmres called without specifying `atol`. The default value will be changed in a future release. For compatibility, specify a value for `atol` explicitly, e.g., ``gmres(..., atol=0)``, or to retain the old behavior ``gmres(..., atol='legacy')``\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run epoch0 with rewards -44.00514531656499\n",
            "MODEL SAVED\n",
            "Run epoch0 with rewards 37.12367591632522\n",
            "MODEL SAVED\n",
            "Run epoch0 with rewards 37.12367591632522\n",
            "MODEL SAVED\n",
            "Run epoch1 with rewards -35.49417217812897\n",
            "Run epoch1 with rewards 21.57311910383176\n",
            "Run epoch1 with rewards 21.57311910383176\n",
            "Run epoch2 with rewards -55.655761643461176\n",
            "Run epoch2 with rewards 36.871460965095764\n",
            "Run epoch2 with rewards 36.871460965095764\n",
            "Run epoch3 with rewards -50.060245029123394\n",
            "Run epoch3 with rewards 30.28143360353078\n",
            "Run epoch3 with rewards 30.28143360353078\n",
            "Run epoch4 with rewards -49.4269331682503\n",
            "Run epoch4 with rewards 4.925641812566234\n",
            "Run epoch4 with rewards 4.925641812566234\n",
            "Run epoch5 with rewards -57.64157478558851\n",
            "Run epoch5 with rewards -9.63305215180371\n",
            "Run epoch5 with rewards -9.63305215180371\n",
            "Run epoch6 with rewards -51.48777451495927\n",
            "Run epoch6 with rewards 22.242134178840356\n",
            "Run epoch6 with rewards 22.242134178840356\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-6f94a8e5e64a>\u001b[0m in \u001b[0;36m<cell line: 44>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0magent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_selection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                 \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_actions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0magent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_selection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pettingzoo/utils/wrappers/order_enforcing.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_updated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAgentID\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mObsType\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pettingzoo/utils/wrappers/base.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mActionType\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_selection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_selection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pettingzoo/utils/wrappers/assert_out_of_bounds.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         ), \"action is not in action space\"\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pettingzoo/utils/wrappers/base.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mActionType\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_selection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_selection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pettingzoo/mpe/_mpe_utils/simple_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"human\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0menable_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"human\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pettingzoo/mpe/_mpe_utils/simple_env.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"human\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "policy_pi = {\"adversary_0\": PolicyPi(8).to(device),\n",
        "            \"agent_0\": PolicyPi(10).to(device),\n",
        "             \"agent_1\": PolicyPi(10).to(device)}\n",
        "\n",
        "gamma = 0.99\n",
        "opts = {\"adversary_0\": SimGD(policy_pi[\"adversary_0\"], lr=0.01),\n",
        "            \"agent_0\": SimGD(policy_pi[\"agent_0\"], lr=0.01),\n",
        "        \"agent_1\": SimGD(policy_pi[\"agent_1\"], lr=0.01)}\n",
        "\n",
        "pcgd = PCGD(policy_pi, 0.3)\n",
        "\n",
        "def pick_sample(s, agent):\n",
        "    with torch.no_grad():\n",
        "        s_batch = np.expand_dims(s, axis=0)\n",
        "        s_batch = torch.tensor(s_batch, dtype=torch.float).to(device)\n",
        "        logits = policy_pi[agent](s_batch)\n",
        "        logits = logits.squeeze(dim=0)\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        a = torch.multinomial(probs, num_samples=1)\n",
        "        return a.tolist()[0]\n",
        "\n",
        "env = simple_adversary_v3.env(render_mode=\"human\")\n",
        "reward_records = defaultdict(lambda : [])\n",
        "\n",
        "def get_monitor():\n",
        "    return torch.concat([p.flatten() for p in policy_pi[\"agent_0\"].parameters()]).flatten()[-1].detach().numpy()\n",
        "\n",
        "monitors = []\n",
        "\n",
        "epochs = 1000\n",
        "batch_size = 2 ** 7\n",
        "\n",
        "def time_ms():\n",
        "    return round(time.time() * 1000)\n",
        "\n",
        "df = pd.DataFrame({\"epoch\": [],\n",
        "                  \"trajectories\": [],\n",
        "                  \"ms\": [],\n",
        "                  \"adversary reward\": [],\n",
        "                  \"agent reward\": []})\n",
        "\n",
        "start_time = time_ms()\n",
        "\n",
        "for i in range(epochs):\n",
        "\n",
        "    # bms = []\n",
        "    zetas = []\n",
        "    loss_mats = []\n",
        "    for j in range(batch_size):\n",
        "        #\n",
        "        # Run episode till done\n",
        "        #\n",
        "        done = False\n",
        "        states = defaultdict(lambda : [])\n",
        "        actions = defaultdict(lambda : [])\n",
        "        rewards = defaultdict(lambda : [])\n",
        "        #s = env.reset()\n",
        "        env.reset(seed=(j * epochs + i))\n",
        "        ss = {}\n",
        "        for agent in env.agents:\n",
        "            env.agent_selection = agent\n",
        "            #print(\"LAST\", env.last()[0].shape)\n",
        "            ss[agent] = env.last()[0]\n",
        "        while not done:\n",
        "            t_actions = {}\n",
        "            for agent in env.agents:\n",
        "                states[agent].append(ss[agent].tolist())\n",
        "                t_actions[agent] = pick_sample(ss[agent], agent)\n",
        "            for agent in env.agents:\n",
        "                env.agent_selection = agent\n",
        "                env.step(t_actions[agent])\n",
        "            for agent in env.agents:\n",
        "                env.agent_selection = agent\n",
        "                s, r, term, trunc, _ = env.last()\n",
        "                ss[agent] = s\n",
        "                done = term or trunc\n",
        "                actions[agent].append(t_actions[agent])\n",
        "                rewards[agent].append(r)\n",
        "\n",
        "        pcgd_log_probs = {}\n",
        "        pcgd_cum_rewards = {}\n",
        "        for agent in env.agents:\n",
        "            cum_rewards = np.zeros_like(rewards[agent])\n",
        "            reward_len = len(rewards[agent])\n",
        "            for j in reversed(range(reward_len)):\n",
        "                cum_rewards[j] = rewards[agent][j] + (cum_rewards[j+1]*gamma if j+1 < reward_len else 0)\n",
        "\n",
        "            #\n",
        "            # Train (optimize parameters)\n",
        "            #\n",
        "            t_states = torch.tensor(states[agent], dtype=torch.float).to(device)\n",
        "            t_actions = torch.tensor(actions[agent], dtype=torch.int64).to(device)\n",
        "            cum_rewards = torch.tensor(cum_rewards, dtype=torch.float).to(device)\n",
        "            logits = policy_pi[agent](t_states).to(device)\n",
        "            log_probs = -F.cross_entropy(logits, t_actions, reduction=\"none\")\n",
        "            loss = -log_probs * cum_rewards\n",
        "\n",
        "            pcgd_log_probs[agent] = log_probs\n",
        "            pcgd_cum_rewards[agent] = cum_rewards\n",
        "\n",
        "        loss_mat = pcgd.loss_matrix(pcgd_log_probs, pcgd_cum_rewards)\n",
        "        zeta = pcgd.zeta(pcgd_log_probs, pcgd_cum_rewards)\n",
        "        loss_mats.append(loss_mat)\n",
        "        zetas.append(zeta)\n",
        "\n",
        "    batch_zeta = torch.stack(zetas, dim=0).mean(dim=0)\n",
        "    batch_loss_mat = torch.stack(loss_mats, dim=0).mean(dim=0)\n",
        "    update = pcgd.compute_loss_mat_update_iterative(batch_loss_mat, batch_zeta)\n",
        "    pcgd.update_parameters(update)\n",
        "\n",
        "    for agent in env.agents:\n",
        "        print(\"Run epoch{} with rewards {}\".format(i, sum(rewards[agent])))\n",
        "        if agent == \"agent_0\":\n",
        "            monitors.append(get_monitor())\n",
        "        if i % 10 == 0:\n",
        "            if agent == \"agent_0\":\n",
        "                ad = pcgd_cum_rewards[\"adversary_0\"][0].detach().numpy()\n",
        "                ag = pcgd_cum_rewards[\"agent_0\"][0].detach().numpy()\n",
        "                df.loc[len(df.index)] = [i, i * batch_size, time_ms() - start_time, ad, ag]\n",
        "                df.to_csv(f\"pcgd_training_metadata_{i}.csv\", index=False)\n",
        "            torch.save(policy_pi[agent], f\"pcgd_{agent}_{i}.model\")\n",
        "            print(\"MODEL SAVED\")\n",
        "        reward_records[agent].append(sum(rewards[agent]))\n",
        "\n",
        "print(\"\\nDone\")\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIkUBJgtO4eR"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axs = plt.subplots(1, 3)\n",
        "fig.suptitle('Simple Spread Reward')\n",
        "plt.xlabel(\"training step\")\n",
        "plt.ylabel(\"cumulative reward\")\n",
        "\n",
        "i = 0\n",
        "for agent in env.agents:\n",
        "    average_reward = []\n",
        "    for idx in range(len(reward_records[agent])):\n",
        "        avg_list = np.empty(shape=(1,), dtype=int)\n",
        "        if idx < 100:\n",
        "            avg_list = reward_records[agent][:idx+1]\n",
        "        else:\n",
        "            avg_list = reward_records[agent][idx-199:idx+1]\n",
        "        average_reward.append(np.average(avg_list))\n",
        "    axs[i].set_title(agent)\n",
        "    axs[i].plot(average_reward, label=\"average reward (last 50 steps)\")\n",
        "    i += 1\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.figure(figsize=(15, 6))\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyO60mR9E3L9KEp89SymFKBg",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}