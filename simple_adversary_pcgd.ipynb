{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wbeard01/PCGD/blob/main/simple_adversary_pcgd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Template / graphing from: https://github.com/tsmatz/reinforcement-learning-tutorials/tree/master"
      ],
      "metadata": {
        "id": "8fGK1QgHCpO3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbLFTuU7rlaW"
      },
      "outputs": [],
      "source": [
        "!pip install pettingzoo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXac27B2rr2x"
      },
      "outputs": [],
      "source": [
        "from pettingzoo.mpe import simple_adversary_v3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJEHNDbrONel"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.sparse.linalg import gmres\n",
        "from scipy.sparse.linalg import LinearOperator\n",
        "import pandas as pd\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Np-QEgcOSUg"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Policy network for agents / adversaries\n",
        "# Fully-connected network with three hidden layers and ReLU activation\n",
        "class PolicyPi(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=256):\n",
        "        super().__init__()\n",
        "\n",
        "        self.firstHidden = nn.Linear(input_dim, hidden_dim)\n",
        "        self.secondHidden = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.thirdHidden = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.classify = nn.Linear(hidden_dim, 5)\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, s):\n",
        "        outs = self.firstHidden(s)\n",
        "        outs = F.relu(outs)\n",
        "        outs = self.secondHidden(outs)\n",
        "        outs = F.relu(outs)\n",
        "        outs = self.thirdHidden(outs)\n",
        "        outs = F.relu(outs)\n",
        "        logits = self.classify(outs)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZsrinkW9WtA"
      },
      "outputs": [],
      "source": [
        "# Simultaneous Gradient Descent (SimGD)\n",
        "class SimGD:\n",
        "    def __init__(self, policy, lr):\n",
        "      self.policy = policy\n",
        "      self.lr = lr\n",
        "\n",
        "    # Empty gradients\n",
        "    def zero_grad(self):\n",
        "        for param in self.policy.parameters():\n",
        "            if param.grad is not None:\n",
        "                param.grad.detach()\n",
        "                param.grad.zero_()\n",
        "\n",
        "    # Update parameters using loss\n",
        "    def step(self, loss):\n",
        "        grads = torch.autograd.grad(loss, self.policy.parameters())\n",
        "        for param, grad in zip(self.policy.parameters(), grads):\n",
        "            param.data -= self.lr * grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykNP2PaJ0dA4"
      },
      "outputs": [],
      "source": [
        "# Polymatrix Competitive Gradient Descent (PCGD)\n",
        "class PCGD:\n",
        "    def __init__(self, policies, eta):\n",
        "        self.policies = policies\n",
        "        self.eta = eta\n",
        "        self.agents = [\"adversary_0\", \"agent_0\", \"agent_1\"]\n",
        "\n",
        "    # Empty gradients\n",
        "    def zero_grad(self):\n",
        "      for agent in self.agents:\n",
        "          for param in self.policies[agent].parameters():\n",
        "              if param.grad is not None:\n",
        "                  param.grad.detach()\n",
        "                  param.grad.zero_()\n",
        "\n",
        "    # Flatten tensors into 1D array\n",
        "    def custom_flatten(self, gp):\n",
        "        flattened = []\n",
        "        for g in gp:\n",
        "            flattened.append(g.flatten())\n",
        "        return torch.concat(flattened)\n",
        "\n",
        "    # Generate matrix of losses for PCGD Hessian\n",
        "    def loss_matrix(self, log_probs, cum_rewards):\n",
        "        agents = self.agents\n",
        "        losses = torch.zeros((len(agents), len(agents)))\n",
        "        for row in range(len(agents)):\n",
        "            for col in range(len(agents)):\n",
        "                losses[row, col] = (cum_rewards[agents[row]] * log_probs[agents[col]] * log_probs[agents[row]]).mean()\n",
        "        return losses\n",
        "\n",
        "    # Vector of first derivatives of loss (recouping SimGD)\n",
        "    def zeta(self, log_probs, cum_rewards):\n",
        "        self.zero_grad()\n",
        "        agents = self.agents\n",
        "        zeta = []\n",
        "        for row in agents:\n",
        "            reward = (log_probs[row] * cum_rewards[row]).mean()\n",
        "            grads = torch.autograd.grad(reward, self.policies[row].parameters(), retain_graph=True, create_graph=True)\n",
        "            zeta.append(self.custom_flatten(grads))\n",
        "        return torch.concat(zeta)\n",
        "\n",
        "    # Matrix-vector product for A = (I + H_o) for solving the linear system in\n",
        "    # PCGD numerically using Krylov subspace methods\n",
        "    def mvp(self, loss_mat, vec):\n",
        "        self.zero_grad()\n",
        "        vec = vec.reshape(-1, 1)\n",
        "        agents = self.agents\n",
        "        split = sum(p.numel() for p in self.policies[agents[0]].parameters())\n",
        "        split2 = sum(p.numel() for p in self.policies[agents[1]].parameters())\n",
        "        blocks = [vec[:split], vec[split:split+split2], vec[split+split2:]]\n",
        "        new_blocks = []\n",
        "        for row in range(len(agents)):\n",
        "            acc = blocks[row].clone()\n",
        "            for col in range(len(agents)):\n",
        "                if row != col:\n",
        "                    reward = loss_mat[row, col]\n",
        "                    grads = self.custom_flatten(torch.autograd.grad(reward, self.policies[agents[col]].parameters(), retain_graph=True, create_graph=True)).reshape(-1, 1)\n",
        "                    vjp = self.custom_flatten(torch.autograd.grad(grads, self.policies[agents[row]].parameters(), [blocks[col]], retain_graph=True)).reshape(-1, 1)\n",
        "                    acc += self.eta * vjp\n",
        "            new_blocks.append(acc)\n",
        "        return torch.concat(new_blocks)\n",
        "\n",
        "    # Solve for PCGD parameter update\n",
        "    def compute_loss_mat_update_iterative(self, loss_mat, zeta):\n",
        "        mv = lambda v: self.mvp(loss_mat, torch.tensor(v)).detach().numpy()\n",
        "        A = LinearOperator((zeta.shape[0], zeta.shape[0]), matvec=mv)\n",
        "        b = zeta.detach().numpy()\n",
        "        return self.eta * torch.tensor(gmres(A, b)[0])\n",
        "\n",
        "    # Magic to safely update parameters\n",
        "    def update_parameters(self, update):\n",
        "        agents = self.agents\n",
        "        split = sum(p.numel() for p in self.policies[agents[0]].parameters())\n",
        "        split2 = sum(p.numel() for p in self.policies[agents[1]].parameters())\n",
        "        first = update[:split]\n",
        "        second = update[split:split+split2]\n",
        "        third = update[split+split2:]\n",
        "        grad_like_policy = []\n",
        "        idx = 0\n",
        "        for param in self.policies[agents[0]].parameters():\n",
        "            grad_like_policy.append(first[idx : idx + torch.numel(param)].reshape(param.shape))\n",
        "            idx += torch.numel(param)\n",
        "        for param, grad in zip(self.policies[agents[0]].parameters(), grad_like_policy):\n",
        "            param.data += grad\n",
        "\n",
        "        grad_like_policy = []\n",
        "        idx = 0\n",
        "        for param in self.policies[agents[1]].parameters():\n",
        "            grad_like_policy.append(second[idx : idx + torch.numel(param)].reshape(param.shape))\n",
        "            idx += torch.numel(param)\n",
        "        for param, grad in zip(self.policies[agents[1]].parameters(), grad_like_policy):\n",
        "            param.data += grad\n",
        "\n",
        "        grad_like_policy = []\n",
        "        idx = 0\n",
        "        for param in self.policies[agents[2]].parameters():\n",
        "            grad_like_policy.append(third[idx : idx + torch.numel(param)].reshape(param.shape))\n",
        "            idx += torch.numel(param)\n",
        "        for param, grad in zip(self.policies[agents[2]].parameters(), grad_like_policy):\n",
        "            param.data += grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6_HlH9hsWUU"
      },
      "outputs": [],
      "source": [
        "# Create and display learning curve for all agents\n",
        "def show_pretty_learning_graph():\n",
        "    fig, axs = plt.subplots(1, 3)\n",
        "    fig.suptitle('Simple Adversary Reward (PCGD)')\n",
        "    plt.xlabel(\"training step\")\n",
        "    plt.ylabel(\"cumulative reward\")\n",
        "    i = 0\n",
        "    for agent in env.agents:\n",
        "        average_reward = []\n",
        "        std = []\n",
        "        for idx in range(len(reward_records[agent])):\n",
        "            avg_list = np.empty(shape=(1,), dtype=int)\n",
        "            if idx < 5:\n",
        "                avg_list = reward_records[agent][:idx+1]\n",
        "            else:\n",
        "                avg_list = reward_records[agent][idx-4:idx+1]\n",
        "            average_reward.append(np.average(avg_list))\n",
        "            std.append(np.std(avg_list))\n",
        "        axs[i].set_title(agent)\n",
        "        axs[i].plot(average_reward, label=\"average reward (last 5 steps)\")\n",
        "        axs[i].fill_between(range(len(reward_records[agent])), np.array(average_reward) - np.array(std),\n",
        "                        np.array(average_reward) + np.array(std), alpha=0.2)\n",
        "        i += 1\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    fig.set_size_inches(22, 10)\n",
        "    plt.show()\n",
        "\n",
        "# Define agents' policies\n",
        "policy_pi = {\"adversary_0\": PolicyPi(8).to(device),\n",
        "            \"agent_0\": PolicyPi(10).to(device),\n",
        "             \"agent_1\": PolicyPi(10).to(device)}\n",
        "\n",
        "# Save optimizers on policies\n",
        "opts = {\"adversary_0\": SimGD(policy_pi[\"adversary_0\"], lr=0.01),\n",
        "            \"agent_0\": SimGD(policy_pi[\"agent_0\"], lr=0.01),\n",
        "            \"agent_1\": SimGD(policy_pi[\"agent_1\"], lr=0.01)}\n",
        "pcgd = PCGD(policy_pi, 0.6)\n",
        "\n",
        "# Training setup\n",
        "env = simple_adversary_v3.env(render_mode=\"rgb_array\")\n",
        "reward_records = defaultdict(lambda : [])\n",
        "batch_size = 2 ** 11\n",
        "epochs = 1001\n",
        "gamma = 0.99\n",
        "\n",
        "# Training metadata\n",
        "df = pd.DataFrame({\"epoch\": [],\n",
        "                  \"trajectories\": [],\n",
        "                  \"adversary reward\": [],\n",
        "                  \"agent reward\": []})\n",
        "\n",
        "# Choose action from policy network\n",
        "def pick_sample(s, agent):\n",
        "    with torch.no_grad():\n",
        "        s_batch = np.expand_dims(s, axis=0)\n",
        "        s_batch = torch.tensor(s_batch, dtype=torch.float).to(device)\n",
        "        logits = policy_pi[agent](s_batch)\n",
        "        logits = logits.squeeze(dim=0)\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        a = torch.multinomial(probs, num_samples=1)\n",
        "        return a.tolist()[0]\n",
        "\n",
        "# Run epochs\n",
        "for i in range(epochs):\n",
        "\n",
        "    # Gradient information for each sample in the epoch\n",
        "    zetas = []\n",
        "    loss_mats = []\n",
        "\n",
        "    # Run batch\n",
        "    for j in range(batch_size):\n",
        "\n",
        "        # Sample from environment\n",
        "        done = False\n",
        "        states = defaultdict(lambda : [])\n",
        "        actions = defaultdict(lambda : [])\n",
        "        rewards = defaultdict(lambda : [])\n",
        "        env.reset(seed=(j * epochs + i))\n",
        "        ss = {}\n",
        "        for agent in env.agents:\n",
        "            env.agent_selection = agent\n",
        "            ss[agent] = env.last()[0]\n",
        "        while not done:\n",
        "            t_actions = {}\n",
        "            for agent in env.agents:\n",
        "                states[agent].append(ss[agent].tolist())\n",
        "                t_actions[agent] = pick_sample(ss[agent], agent)\n",
        "            for agent in env.agents:\n",
        "                env.agent_selection = agent\n",
        "                env.step(t_actions[agent])\n",
        "            for agent in env.agents:\n",
        "                env.agent_selection = agent\n",
        "                s, r, term, trunc, _ = env.last()\n",
        "                ss[agent] = s\n",
        "                done = term or trunc\n",
        "                actions[agent].append(t_actions[agent])\n",
        "                rewards[agent].append(r)\n",
        "\n",
        "        # Save optimization information\n",
        "        pcgd_log_probs = {}\n",
        "        pcgd_cum_rewards = {}\n",
        "        for agent in env.agents:\n",
        "            cum_rewards = np.zeros_like(rewards[agent])\n",
        "            reward_len = len(rewards[agent])\n",
        "            for j in reversed(range(reward_len)):\n",
        "                cum_rewards[j] = rewards[agent][j] + (cum_rewards[j+1]*gamma if j+1 < reward_len else 0)\n",
        "\n",
        "            t_states = torch.tensor(states[agent], dtype=torch.float).to(device)\n",
        "            t_actions = torch.tensor(actions[agent], dtype=torch.int64).to(device)\n",
        "            cum_rewards = torch.tensor(cum_rewards, dtype=torch.float).to(device)\n",
        "            logits = policy_pi[agent](t_states).to(device)\n",
        "            log_probs = -F.cross_entropy(logits, t_actions, reduction=\"none\")\n",
        "            loss = -log_probs * cum_rewards\n",
        "\n",
        "            pcgd_log_probs[agent] = log_probs\n",
        "            pcgd_cum_rewards[agent] = cum_rewards\n",
        "\n",
        "        # Record losses for PCGD\n",
        "        loss_mat = pcgd.loss_matrix(pcgd_log_probs, pcgd_cum_rewards)\n",
        "        zeta = pcgd.zeta(pcgd_log_probs, pcgd_cum_rewards)\n",
        "        loss_mats.append(loss_mat)\n",
        "        zetas.append(zeta)\n",
        "\n",
        "    # Perform PCGD update\n",
        "    batch_zeta = torch.stack(zetas, dim=0).mean(dim=0)\n",
        "    batch_loss_mat = torch.stack(loss_mats, dim=0).mean(dim=0)\n",
        "    update = pcgd.compute_loss_mat_update_iterative(batch_loss_mat, batch_zeta)\n",
        "    pcgd.update_parameters(update)\n",
        "\n",
        "    # Save details / models as appropriate\n",
        "    for agent in env.agents:\n",
        "        print(\"Run epoch{} with rewards {}\".format(i, sum(rewards[agent])))\n",
        "        if agent == \"agent_0\":\n",
        "            ad = pcgd_cum_rewards[\"adversary_0\"][0].detach().numpy()\n",
        "            ag = pcgd_cum_rewards[\"agent_0\"][0].detach().numpy()\n",
        "            df.loc[len(df.index)] = [i, i * batch_size, ad, ag]\n",
        "            df.to_csv(f\"pcgd_training_metadata_{i}.csv\", index=False)\n",
        "        torch.save(policy_pi[agent], f\"pcgd_{agent}_{i}.model\")\n",
        "        print(\"MODEL SAVED\")\n",
        "        reward_records[agent].append(sum(rewards[agent]))\n",
        "        show_pretty_learning_graph()\n",
        "\n",
        "print(\"\\nDone\")\n",
        "env.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyMzcxzD3g9ULE7/b5d+BNp6",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}